{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting is a powerful machine learning algorithm that builds models sequentially, where each subsequent model corrects the errors of its predecessor. It combines multiple weak learners, usually decision trees, into a strong predictive model. Here's an overview:\n",
    "\n",
    "## Key Concepts in Gradient Boosting:\n",
    "Base Learners:\n",
    "\n",
    "Typically, decision trees with a shallow depth (e.g., stumps with 1–3 splits) are used.\n",
    "These weak models are trained iteratively.\n",
    "Gradient Descent:\n",
    "\n",
    "Gradient Boosting uses the concept of gradient descent to minimize the loss function by adding trees that correct previous errors.\n",
    "The gradient of the loss function with respect to the predictions is calculated, and new trees are built to approximate these gradients.\n",
    "Loss Function:\n",
    "\n",
    "The algorithm is flexible and can work with various loss functions:\n",
    "Mean Squared Error (MSE) for regression tasks.\n",
    "Log Loss for binary classification.\n",
    "Deviance for multi-class classification.\n",
    "Ensemble Approach:\n",
    "\n",
    "Each new tree improves the overall model by correcting residuals from the previous step.\n",
    "#### Steps in Gradient Boosting:\n",
    "Initialize the model with a simple prediction (e.g., the mean value for regression or the log-odds for classification).\n",
    "Compute residuals based on the current model's predictions.\n",
    "Fit a weak learner (decision tree) to the residuals.\n",
    "Update the model's predictions by adding the new learner's contribution, scaled by a learning rate.\n",
    "Repeat steps 2–4 for a specified number of iterations or until convergence.\n",
    "Key Hyperparameters:\n",
    "Learning Rate (eta): Controls the contribution of each tree. Smaller values require more iterations but improve generalization.\n",
    "Number of Trees (n_estimators): Determines how many trees will be built. A high value with early stopping is common.\n",
    "Tree Depth (max_depth): Controls the complexity of individual trees.\n",
    "Subsample: Fraction of samples used for training each tree (reduces overfitting).\n",
    "Loss Function: Determines the objective to minimize.\n",
    "Popular Implementations:\n",
    "Scikit-learn's GradientBoostingClassifier and GradientBoostingRegressor:\n",
    "Easy-to-use but slower on large datasets.\n",
    "XGBoost:\n",
    "Highly optimized and supports regularization.\n",
    "LightGBM:\n",
    "Focuses on efficiency and handles large datasets.\n",
    "CatBoost:\n",
    "Specializes in categorical features with minimal preprocessing.\n",
    "### Use Cases:\n",
    "Predictive modeling in finance (e.g., credit scoring).\n",
    "Feature importance analysis.\n",
    "Recommender systems.\n",
    "Anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9862402280647262\n",
      "Mean Squared Error: 0.36\n",
      "Feature Importances: [0.91749634 0.06675946 0.01225596 0.00190831 0.00157993]\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 5)  # 1000 samples, 5 features\n",
    "y = X[:, 0] * 10 + np.sin(X[:, 1] * 5) - X[:, 2]**2 + np.random.normal(0, 0.5, 1000)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=100,    # Number of trees\n",
    "    learning_rate=0.1,   # Step size shrinkage\n",
    "    max_depth=3,         # Maximum depth of each tree\n",
    "    subsample=0.8,       # Fraction of samples used for each tree\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gbr.predict(X_test)\n",
    "print(gbr.score(X_train, y_train))\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "\n",
    "# Feature importances\n",
    "print(\"Feature Importances:\", gbr.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
