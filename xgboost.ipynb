{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost (Extreme Gradient Boosting) is an optimized implementation of gradient boosting that is designed for speed and performance. It is widely used in machine learning competitions and for production models due to its scalability and efficiency.\n",
    "\n",
    "### Key Features of XGBoost:\n",
    "Optimized for Speed:\n",
    "Uses parallel processing and efficient tree-building techniques.\n",
    "### Regularization:\n",
    "Includes L1 and L2 regularization to prevent overfitting.\n",
    "Sparsity Awareness:\n",
    "Handles missing values natively by learning their best split direction.\n",
    "### Flexibility:\n",
    "Works with custom loss functions.\n",
    "Boosting with Pruning:\n",
    "Uses \"max_depth\" and \"eta\" for better control over tree growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99875\n",
      "Accuracy: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kumaw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [12:25:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Generate synthetic dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 5)  # 1000 samples, 5 features\n",
    "y = (X[:, 0] + np.sin(X[:, 1] * 5) > 0.5).astype(int)  # Binary classification\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost Classifier\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=100,         # Number of trees\n",
    "    learning_rate=0.1,        # Step size shrinkage\n",
    "    max_depth=3,              # Maximum depth of each tree\n",
    "    subsample=0.8,            # Fraction of samples for training\n",
    "    colsample_bytree=0.8,     # Fraction of features for training\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,  # Necessary for recent versions of XGBoost\n",
    "    eval_metric=\"logloss\"     # Evaluation metric\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb.fit(X_train, y_train)\n",
    "print(xgb.score(X_train,y_train))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Feature importance\n",
    "# import matplotlib.pyplot as plt\n",
    "# xgb.plot_importance(xgb)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
